{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "#import pycocotools.mask as mask_util\n",
    "import torch\n",
    "from torch import device\n",
    "\n",
    "#from detectron2.layers.roi_align import ROIAlign\n",
    "#from detectron2.utils.memory import retry_if_cuda_oom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = (1716, 942)\n",
    "root = os.path.join('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_masks(filepath='./SEG_Train_Datasets/Train_Annotations'):\n",
    "    '''\n",
    "    Read masks from directory and tranform to categorical\n",
    "    '''\n",
    "    file_list = [file for file in os.listdir(filepath) if file.endswith('.json')]\n",
    "    file_list.sort()\n",
    "    n_masks = len(file_list)\n",
    "    masks = np.empty((n_masks, 1716, 942), dtype=bool)\n",
    "\n",
    "    for i, file in enumerate(file_list):\n",
    "        with open(filepath+'/'+file, newline='') as jsonfile:\n",
    "            data = json.load(jsonfile)\n",
    "            for i in range(len(data['shapes'])):\n",
    "                for j in range(len(data['shapes'][i]['points'])):\n",
    "                    #print(int(data['shapes'][i]['points'][j][0]), int(data['shapes'][i]['points'][j][1]))\n",
    "                    masks[i, int(data['shapes'][i]['points'][j][0]), int(data['shapes'][i]['points'][j][1])] = 1\n",
    "\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('./SEG_Train_Datasets/Train_Annotations/00001002.json', newline='') as jsonfile:\\n    data = json.load(jsonfile)\\n    masks = np.empty((1716, 942))\\n    masks[int(data['shapes'][1]['points'][0][0]), int(data['shapes'][1]['points'][0][1])] = 1\\n    #print(data)\\n    #print(len(data['shapes']))\\n    #print(data['shapes'][1]['points'][0][0])\\n    #print(len(data['shapes'][1]['points']))\\n    #print(masks)\\n    for i in range(len(data['shapes'])):\\n        for j in range(len(data['shapes'][i]['points'])):\\n            #print(int(data['shapes'][i]['points'][j][0]), int(data['shapes'][i]['points'][j][1]))\\n            masks[int(data['shapes'][i]['points'][j][0]), int(data['shapes'][i]['points'][j][1])] = 1\\n\\n \\n\\n    print(masks)\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "'''\n",
    "with open('./SEG_Train_Datasets/Train_Annotations/00001002.json', newline='') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "    masks = np.empty((1716, 942))\n",
    "    masks[int(data['shapes'][1]['points'][0][0]), int(data['shapes'][1]['points'][0][1])] = 1\n",
    "    #print(data)\n",
    "    #print(len(data['shapes']))\n",
    "    #print(data['shapes'][1]['points'][0][0])\n",
    "    #print(len(data['shapes'][1]['points']))\n",
    "    #print(masks)\n",
    "    for i in range(len(data['shapes'])):\n",
    "        for j in range(len(data['shapes'][i]['points'])):\n",
    "            #print(int(data['shapes'][i]['points'][j][0]), int(data['shapes'][i]['points'][j][1]))\n",
    "            masks[int(data['shapes'][i]['points'][j][0]), int(data['shapes'][i]['points'][j][1])] = 1\n",
    "\n",
    " \n",
    "\n",
    "    print(masks)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDs(Dataset):\n",
    "    def __init__(self, root='.', train_test='train', is_val=False, train_val_split=False, val_split_rate=0.9,\n",
    "                 transform=None):\n",
    "        self.filenames = []\n",
    "        self.labels = np.array([])\n",
    "        self.len = 0\n",
    "        self.transform = transform\n",
    "        self.train_test = train_test\n",
    "        if train_test == 'test':\n",
    "            file_path = os.path.join(root, 'Public_Image')\n",
    "        else:\n",
    "            file_path = os.path.join(root,  'SEG_Train_Datasets', 'Train_Images')\n",
    "            #print('file path is', file_path)\n",
    "            #         label\n",
    "            if os.path.isfile(train_test + '_label_cat.npy'):\n",
    "                self.labels = np.load(train_test + '_label_cat.npy')\n",
    "                print(self.labels.shape)\n",
    "            else:\n",
    "                self.labels = read_masks()\n",
    "                # print(type(self.labels))\n",
    "                print(self.labels.shape)\n",
    "                np.save(train_test + '_label_cat.npy', self.labels)\n",
    "            # data\n",
    "        self.filenames = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith('.jpg')]\n",
    "\n",
    "        self.filenames.sort()\n",
    "        if train_val_split:\n",
    "            split_nums = int(len(self.filenames) * val_split_rate)\n",
    "            self.filenames = self.filenames[split_nums:] if is_val else self.filenames[:split_nums]\n",
    "            self.labels = self.labels[split_nums:] if is_val else self.labels[:split_nums]\n",
    "        self.len = len(self.filenames)\n",
    "        # print(self.filenames[808])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.filenames[index]\n",
    "        image = Image.open(image_fn)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.train_test == 'test':\n",
    "            return image\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n",
      "Device used: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(torch.__version__)\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print('Device used:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1053, 1716, 942)\n"
     ]
    }
   ],
   "source": [
    "train_set = SegDs(train_test='train', is_val=False, train_val_split=False, transform=transforms.Compose([\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
